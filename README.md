# ğŸ›’ Automated E-Commerce ETL Pipeline  

## ğŸ“Œ Project Overview  
This project demonstrates a modern **end-to-end data engineering pipeline** built with:  

- âœ… **Python** â€” for generating dummy e-commerce data (customers, orders, shipments).  
- âœ… **Snowflake** â€” as the cloud data warehouse.  
- âœ… **dbt** â€” for transformations (raw â†’ analytics).  
- âœ… **Apache Airflow** â€” for orchestration and scheduling.  

The pipeline simulates an **E-commerce order monitoring system**, taking raw data through ingestion, transformation, and analytics-ready modeling.  

---

## ğŸ—ï¸ Architecture  
1. **Data Generation**: Python scripts create synthetic e-commerce datasets (CSV format).  
2. **Raw Layer**: Data is loaded into a `raw` schema in Snowflake.  
3. **Transformations**: dbt models clean, join, and structure the data into fact & dimension tables.  
4. **Analytics Layer**: Results are materialized into an `analytics` schema.  
5. **Orchestration**: Airflow DAG schedules the pipeline.  

---

## âš™ï¸ Tech Stack  
- **Language**: Python 3.9  
- **Warehouse**: Snowflake  
- **Transformations**: dbt  
- **Orchestration**: Apache Airflow  
- **Version Control**: GitHub  

---

## ğŸš€ Setup & Installation  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/NdukaClara/automated-ecommerce-etl-pipeline.git
cd automated-ecommerce-etl-pipeline
```

### 2ï¸âƒ£ Create a Virtual Environment
```bash
python3 -m venv airflow_venv_1
source airflow_venv_1/bin/activate
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure dbt (Snowflake Profile)

Edit your ~/.dbt/profiles.yml with Snowflake credentials:
```yaml
ecommerce_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: <account>
      user: <user>
      password: <password>
      role: <role>
      database: ecommerce_db
      warehouse: <warehouse>
      schema: raw
```

### 5ï¸âƒ£ Run dbt Models
```bash
dbt run
```

### 6ï¸âƒ£ Run Airflow (Standalone Mode for Dev)
```bash
airflow standalone
```

## ğŸ“Š Example dbt Models

stg_orders â†’ staging table

dim_customers â†’ dimension table

fct_orders â†’ fact table

## ğŸ“ Project Structure
```plaintext
ecommerce_etl_project/
â”‚â”€â”€ dags/                # Airflow DAGs  
â”‚â”€â”€ dbt/                 # dbt models  
â”‚â”€â”€ data/                # Generated CSVs  
â”‚â”€â”€ scripts/             # Python data generator  
â”‚â”€â”€ airflow_venv_1/      # Virtual environment (excluded in .gitignore)  
â”‚â”€â”€ requirements.txt     # Dependencies  
â”‚â”€â”€ README.md            # Project documentation  
```

## ğŸ”® Future Improvements

Add monitoring with Great Expectations

Add dbt tests for data quality

Deploy Airflow with Docker Compose

## ğŸ™Œ Acknowledgments

This project was built as part of my journey into Data Engineering, focusing on practical, real-world pipelines.
